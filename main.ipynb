{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Network with 30 hidden Neurons Using Cuadratic Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import network\n",
    "net = network.Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stochastic gradient descent to learn from training_data for 30 epochs\n",
    "### Mini-batch size 10 and eta= 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7958 / 10000, took 9.01 seconds\n",
      "Epoch 1: 9242 / 10000, took 8.83 seconds\n",
      "Epoch 2: 9305 / 10000, took 8.79 seconds\n",
      "Epoch 3: 9347 / 10000, took 9.05 seconds\n",
      "Epoch 4: 9385 / 10000, took 8.94 seconds\n",
      "Epoch 5: 9415 / 10000, took 8.77 seconds\n",
      "Epoch 6: 9395 / 10000, took 8.73 seconds\n",
      "Epoch 7: 9445 / 10000, took 8.67 seconds\n",
      "Epoch 8: 9443 / 10000, took 8.69 seconds\n",
      "Epoch 9: 9445 / 10000, took 9.18 seconds\n",
      "Epoch 10: 9445 / 10000, took 8.74 seconds\n",
      "Epoch 11: 9436 / 10000, took 8.79 seconds\n",
      "Epoch 12: 9491 / 10000, took 9.13 seconds\n",
      "Epoch 13: 9455 / 10000, took 9.44 seconds\n",
      "Epoch 14: 9483 / 10000, took 9.53 seconds\n",
      "Epoch 15: 9437 / 10000, took 9.85 seconds\n",
      "Epoch 16: 9499 / 10000, took 10.40 seconds\n",
      "Epoch 17: 9471 / 10000, took 9.58 seconds\n",
      "Epoch 18: 9496 / 10000, took 8.78 seconds\n",
      "Epoch 19: 9475 / 10000, took 8.81 seconds\n",
      "Epoch 20: 9482 / 10000, took 8.94 seconds\n",
      "Epoch 21: 9461 / 10000, took 8.72 seconds\n",
      "Epoch 22: 9511 / 10000, took 8.68 seconds\n",
      "Epoch 23: 9509 / 10000, took 8.96 seconds\n",
      "Epoch 24: 9507 / 10000, took 8.79 seconds\n",
      "Epoch 25: 9535 / 10000, took 8.70 seconds\n",
      "Epoch 26: 9499 / 10000, took 8.70 seconds\n",
      "Epoch 27: 9499 / 10000, took 8.97 seconds\n",
      "Epoch 28: 9495 / 10000, took 9.04 seconds\n",
      "Epoch 29: 9526 / 10000, took 8.78 seconds\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar but with 100 hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 5684 / 10000, took 30.32 seconds\n",
      "Epoch 1: 6576 / 10000, took 32.66 seconds\n",
      "Epoch 2: 7610 / 10000, took 32.14 seconds\n",
      "Epoch 3: 8541 / 10000, took 30.30 seconds\n",
      "Epoch 4: 8555 / 10000, took 29.91 seconds\n",
      "Epoch 5: 8600 / 10000, took 32.29 seconds\n",
      "Epoch 6: 8634 / 10000, took 32.67 seconds\n",
      "Epoch 7: 8650 / 10000, took 29.98 seconds\n",
      "Epoch 8: 8662 / 10000, took 30.83 seconds\n",
      "Epoch 9: 8670 / 10000, took 32.14 seconds\n",
      "Epoch 10: 8678 / 10000, took 32.25 seconds\n",
      "Epoch 11: 8681 / 10000, took 30.18 seconds\n",
      "Epoch 12: 8689 / 10000, took 29.99 seconds\n",
      "Epoch 13: 8670 / 10000, took 31.53 seconds\n",
      "Epoch 14: 8698 / 10000, took 40.39 seconds\n",
      "Epoch 15: 8696 / 10000, took 48.81 seconds\n",
      "Epoch 16: 8699 / 10000, took 48.63 seconds\n",
      "Epoch 17: 8706 / 10000, took 50.80 seconds\n",
      "Epoch 18: 8713 / 10000, took 52.83 seconds\n",
      "Epoch 19: 8694 / 10000, took 48.28 seconds\n",
      "Epoch 20: 8705 / 10000, took 47.24 seconds\n",
      "Epoch 21: 8710 / 10000, took 49.15 seconds\n",
      "Epoch 22: 8708 / 10000, took 52.95 seconds\n",
      "Epoch 23: 8701 / 10000, took 49.67 seconds\n",
      "Epoch 24: 8708 / 10000, took 48.51 seconds\n",
      "Epoch 25: 8715 / 10000, took 50.74 seconds\n",
      "Epoch 26: 8719 / 10000, took 49.23 seconds\n",
      "Epoch 27: 8710 / 10000, took 47.59 seconds\n",
      "Epoch 28: 8728 / 10000, took 48.02 seconds\n",
      "Epoch 29: 8713 / 10000, took 49.48 seconds\n"
     ]
    }
   ],
   "source": [
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No hidden neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1135 / 10000, took 2.62 seconds\n",
      "Epoch 1: 980 / 10000, took 2.56 seconds\n",
      "Epoch 2: 1135 / 10000, took 2.54 seconds\n",
      "Epoch 3: 1135 / 10000, took 2.59 seconds\n",
      "Epoch 4: 1032 / 10000, took 2.60 seconds\n",
      "Epoch 5: 1032 / 10000, took 2.67 seconds\n",
      "Epoch 6: 980 / 10000, took 2.58 seconds\n",
      "Epoch 7: 1135 / 10000, took 2.56 seconds\n",
      "Epoch 8: 958 / 10000, took 2.55 seconds\n",
      "Epoch 9: 1010 / 10000, took 2.56 seconds\n",
      "Epoch 10: 1009 / 10000, took 2.55 seconds\n",
      "Epoch 11: 958 / 10000, took 2.73 seconds\n",
      "Epoch 12: 958 / 10000, took 2.70 seconds\n",
      "Epoch 13: 1010 / 10000, took 2.64 seconds\n",
      "Epoch 14: 980 / 10000, took 2.55 seconds\n",
      "Epoch 15: 1009 / 10000, took 2.53 seconds\n",
      "Epoch 16: 982 / 10000, took 2.57 seconds\n",
      "Epoch 17: 1135 / 10000, took 2.53 seconds\n",
      "Epoch 18: 1135 / 10000, took 2.54 seconds\n",
      "Epoch 19: 1010 / 10000, took 2.55 seconds\n",
      "Epoch 20: 982 / 10000, took 2.65 seconds\n",
      "Epoch 21: 1135 / 10000, took 2.59 seconds\n",
      "Epoch 22: 1028 / 10000, took 2.55 seconds\n",
      "Epoch 23: 1028 / 10000, took 2.54 seconds\n",
      "Epoch 24: 1135 / 10000, took 2.57 seconds\n",
      "Epoch 25: 1032 / 10000, took 2.54 seconds\n",
      "Epoch 26: 1028 / 10000, took 2.53 seconds\n",
      "Epoch 27: 974 / 10000, took 2.54 seconds\n",
      "Epoch 28: 1135 / 10000, took 2.58 seconds\n",
      "Epoch 29: 1028 / 10000, took 2.53 seconds\n"
     ]
    }
   ],
   "source": [
    "net = network.Network([784, 0, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a similar manner we investigate the behaviour of our network with a cross entropy cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9073 / 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m net \u001b[38;5;241m=\u001b[39m network2\u001b[38;5;241m.\u001b[39mNetwork(([\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m10\u001b[39m]), cost\u001b[38;5;241m=\u001b[39mnetwork2\u001b[38;5;241m.\u001b[39mCrossEntropyCost) \u001b[38;5;66;03m#30 hidden layers for Cross Entropy Cost function\u001b[39;00m\n\u001b[0;32m      4\u001b[0m net\u001b[38;5;241m.\u001b[39mlarge_weight_initializer()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_evaluation_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\David\\Desktop\\programming_projects\\mnist_NN\\neural-networks-and-deep-learning\\src\\network2.py:165\u001b[0m, in \u001b[0;36mNetwork.SGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, lmbda, evaluation_data, monitor_evaluation_cost, monitor_evaluation_accuracy, monitor_training_cost, monitor_training_accuracy)\u001b[0m\n\u001b[0;32m    161\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    162\u001b[0m     training_data[k:k\u001b[38;5;241m+\u001b[39mmini_batch_size]\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n, mini_batch_size)]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_mini_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmbda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m training complete\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m j)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m monitor_training_cost:\n",
      "File \u001b[1;32mc:\\Users\\David\\Desktop\\programming_projects\\mnist_NN\\neural-networks-and-deep-learning\\src\\network2.py:201\u001b[0m, in \u001b[0;36mNetwork.update_mini_batch\u001b[1;34m(self, mini_batch, eta, lmbda, n)\u001b[0m\n\u001b[0;32m    199\u001b[0m nabla_w \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(w\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m mini_batch:\n\u001b[1;32m--> 201\u001b[0m     delta_nabla_b, delta_nabla_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     nabla_b \u001b[38;5;241m=\u001b[39m [nb\u001b[38;5;241m+\u001b[39mdnb \u001b[38;5;28;01mfor\u001b[39;00m nb, dnb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nabla_b, delta_nabla_b)]\n\u001b[0;32m    203\u001b[0m     nabla_w \u001b[38;5;241m=\u001b[39m [nw\u001b[38;5;241m+\u001b[39mdnw \u001b[38;5;28;01mfor\u001b[39;00m nw, dnw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nabla_w, delta_nabla_w)]\n",
      "File \u001b[1;32mc:\\Users\\David\\Desktop\\programming_projects\\mnist_NN\\neural-networks-and-deep-learning\\src\\network2.py:215\u001b[0m, in \u001b[0;36mNetwork.backprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03mgradient for the cost function C_x.  ``nabla_b`` and\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m``nabla_w`` are layer-by-layer lists of numpy arrays, similar\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03mto ``self.biases`` and ``self.weights``.\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m nabla_b \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(b\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases]\n\u001b[1;32m--> 215\u001b[0m nabla_w \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(w\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# feedforward\u001b[39;00m\n\u001b[0;32m    217\u001b[0m activation \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\David\\Desktop\\programming_projects\\mnist_NN\\neural-networks-and-deep-learning\\src\\network2.py:215\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03mgradient for the cost function C_x.  ``nabla_b`` and\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m``nabla_w`` are layer-by-layer lists of numpy arrays, similar\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03mto ``self.biases`` and ``self.weights``.\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m nabla_b \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(b\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases]\n\u001b[1;32m--> 215\u001b[0m nabla_w \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# feedforward\u001b[39;00m\n\u001b[0;32m    217\u001b[0m activation \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src import network2\n",
    "\n",
    "net = network2.Network(([784, 30, 10]), cost=network2.CrossEntropyCost) #30 hidden layers for Cross Entropy Cost function\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 100 hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9101 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9269 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9292 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9390 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9428 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9416 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9458 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9442 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9472 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9486 / 10000\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9495 / 10000\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9508 / 10000\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9507 / 10000\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9524 / 10000\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9534 / 10000\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9504 / 10000\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9517 / 10000\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9525 / 10000\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9535 / 10000\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9503 / 10000\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9520 / 10000\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9519 / 10000\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9525 / 10000\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9541 / 10000\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9534 / 10000\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9528 / 10000\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9507 / 10000\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9533 / 10000\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9539 / 10000\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9528 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9101,\n",
       "  9269,\n",
       "  9292,\n",
       "  9390,\n",
       "  9428,\n",
       "  9416,\n",
       "  9458,\n",
       "  9442,\n",
       "  9472,\n",
       "  9486,\n",
       "  9495,\n",
       "  9508,\n",
       "  9507,\n",
       "  9524,\n",
       "  9534,\n",
       "  9504,\n",
       "  9517,\n",
       "  9525,\n",
       "  9535,\n",
       "  9503,\n",
       "  9520,\n",
       "  9519,\n",
       "  9525,\n",
       "  9541,\n",
       "  9534,\n",
       "  9528,\n",
       "  9507,\n",
       "  9533,\n",
       "  9539,\n",
       "  9528],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = network2.Network(([784, 30, 10]), cost=network2.CrossEntropyCost) #30 hidden layers for Cross Entropy Cost function\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis of the networks and algorithms Overfitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1.8945396041045133\n",
      "Accuracy on evaluation data: 5837 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.3953496565015362\n",
      "Accuracy on evaluation data: 6819 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1991417703682612\n",
      "Accuracy on evaluation data: 7128 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 1.0126872783903407\n",
      "Accuracy on evaluation data: 7468 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.8419328491914003\n",
      "Accuracy on evaluation data: 7808 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.775431969469158\n",
      "Accuracy on evaluation data: 7846 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.6838249069147905\n",
      "Accuracy on evaluation data: 7932 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.6160513111299628\n",
      "Accuracy on evaluation data: 8017 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.5444903990460784\n",
      "Accuracy on evaluation data: 8122 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.5323828173446684\n",
      "Accuracy on evaluation data: 8029 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.4648742818701191\n",
      "Accuracy on evaluation data: 8138 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.4141119158372673\n",
      "Accuracy on evaluation data: 8176 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.3718897159739673\n",
      "Accuracy on evaluation data: 8218 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.34695173949443264\n",
      "Accuracy on evaluation data: 8214 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.32335882523801235\n",
      "Accuracy on evaluation data: 8234 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.30492712483643863\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.2867598325713026\n",
      "Accuracy on evaluation data: 8222 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.2692215844397362\n",
      "Accuracy on evaluation data: 8248 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.24142734479076114\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.22421913765983556\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.21894352172935855\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.20855393429758307\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.18692501290336042\n",
      "Accuracy on evaluation data: 8310 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.1782425942950656\n",
      "Accuracy on evaluation data: 8306 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.1677525617504816\n",
      "Accuracy on evaluation data: 8322 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.16030858788839275\n",
      "Accuracy on evaluation data: 8322 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.15406312057681468\n",
      "Accuracy on evaluation data: 8321 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.1442880006843516\n",
      "Accuracy on evaluation data: 8330 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.1372163976653251\n",
      "Accuracy on evaluation data: 8323 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.13162827830505927\n",
      "Accuracy on evaluation data: 8334 / 10000\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.12735270544714947\n",
      "Accuracy on evaluation data: 8342 / 10000\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.11947483393396191\n",
      "Accuracy on evaluation data: 8338 / 10000\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.11477642525570472\n",
      "Accuracy on evaluation data: 8331 / 10000\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.11250635610587101\n",
      "Accuracy on evaluation data: 8334 / 10000\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.10825633683310873\n",
      "Accuracy on evaluation data: 8342 / 10000\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.10215740471515891\n",
      "Accuracy on evaluation data: 8348 / 10000\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.09846656851438527\n",
      "Accuracy on evaluation data: 8334 / 10000\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.09649603827789266\n",
      "Accuracy on evaluation data: 8351 / 10000\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.0912575354567861\n",
      "Accuracy on evaluation data: 8342 / 10000\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.08970993846601422\n",
      "Accuracy on evaluation data: 8348 / 10000\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.08538456385624618\n",
      "Accuracy on evaluation data: 8333 / 10000\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.08121847831481757\n",
      "Accuracy on evaluation data: 8338 / 10000\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.07984729518961645\n",
      "Accuracy on evaluation data: 8350 / 10000\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.07592777990665636\n",
      "Accuracy on evaluation data: 8344 / 10000\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.0735881751747304\n",
      "Accuracy on evaluation data: 8340 / 10000\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.07104001715966438\n",
      "Accuracy on evaluation data: 8349 / 10000\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.06829898457384864\n",
      "Accuracy on evaluation data: 8341 / 10000\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.06610886085224928\n",
      "Accuracy on evaluation data: 8351 / 10000\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.06423825224270642\n",
      "Accuracy on evaluation data: 8345 / 10000\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.06283370027875229\n",
      "Accuracy on evaluation data: 8357 / 10000\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.06060944982781599\n",
      "Accuracy on evaluation data: 8350 / 10000\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.05915456132334416\n",
      "Accuracy on evaluation data: 8356 / 10000\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.05787387072242858\n",
      "Accuracy on evaluation data: 8354 / 10000\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.056380773991726606\n",
      "Accuracy on evaluation data: 8351 / 10000\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.05464500578511769\n",
      "Accuracy on evaluation data: 8362 / 10000\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.05338360987972593\n",
      "Accuracy on evaluation data: 8359 / 10000\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.052689702217370044\n",
      "Accuracy on evaluation data: 8369 / 10000\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.05087934430038287\n",
      "Accuracy on evaluation data: 8362 / 10000\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.04948735871520646\n",
      "Accuracy on evaluation data: 8359 / 10000\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.04830192042471011\n",
      "Accuracy on evaluation data: 8362 / 10000\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.04798296543281493\n",
      "Accuracy on evaluation data: 8357 / 10000\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.046328549951913794\n",
      "Accuracy on evaluation data: 8363 / 10000\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.04539127456704237\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.04433174255264326\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.04371379734385616\n",
      "Accuracy on evaluation data: 8367 / 10000\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.042525379460731254\n",
      "Accuracy on evaluation data: 8376 / 10000\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.04161170296127121\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.04069434049957627\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.03970637881424219\n",
      "Accuracy on evaluation data: 8373 / 10000\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.03898764333702563\n",
      "Accuracy on evaluation data: 8366 / 10000\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.03818297065432241\n",
      "Accuracy on evaluation data: 8373 / 10000\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.037455404750470066\n",
      "Accuracy on evaluation data: 8372 / 10000\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.03664210200177689\n",
      "Accuracy on evaluation data: 8365 / 10000\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.03602104221023731\n",
      "Accuracy on evaluation data: 8363 / 10000\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.03533446794152662\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.03466184270029866\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.034311124743951496\n",
      "Accuracy on evaluation data: 8361 / 10000\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.033433035982893024\n",
      "Accuracy on evaluation data: 8366 / 10000\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.03278921041374515\n",
      "Accuracy on evaluation data: 8369 / 10000\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.03228441053463231\n",
      "Accuracy on evaluation data: 8369 / 10000\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.031639095351728125\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.03113491388586037\n",
      "Accuracy on evaluation data: 8367 / 10000\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.030583750262745305\n",
      "Accuracy on evaluation data: 8372 / 10000\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.02999356984401455\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.029501432791083054\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.028998006387209806\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.028507100111852292\n",
      "Accuracy on evaluation data: 8371 / 10000\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.02802727496448332\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.027535194386568297\n",
      "Accuracy on evaluation data: 8373 / 10000\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.027045888274652218\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.026715327922240992\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.02619613905385415\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.025796150587489113\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.02534751213319773\n",
      "Accuracy on evaluation data: 8372 / 10000\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.02494083488674298\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.02461471279594119\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.024153438756269355\n",
      "Accuracy on evaluation data: 8371 / 10000\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.02384150585793765\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.02348425697619531\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.023171397130190643\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.022805204320697475\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.022468803769767082\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.02216801632552247\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.021961938017286183\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.021557009158748298\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.021262581934206794\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.020996247079719723\n",
      "Accuracy on evaluation data: 8371 / 10000\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.020737781693415735\n",
      "Accuracy on evaluation data: 8364 / 10000\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.020531688089895483\n",
      "Accuracy on evaluation data: 8369 / 10000\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.020302760076465334\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.019980651504799243\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.019682623865699876\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.019449402088868625\n",
      "Accuracy on evaluation data: 8373 / 10000\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.019235425505213632\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.019002439448171735\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.01875273510571845\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.01852292033719572\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.018330783759363537\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.01811080517027363\n",
      "Accuracy on evaluation data: 8373 / 10000\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.01792685851519484\n",
      "Accuracy on evaluation data: 8376 / 10000\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.017710344396465037\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.017490910924275432\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.017304514733114514\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.01710475519489045\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.016918810369400822\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.0167308887409212\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.0165635764686611\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.016389108971073926\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.016209723713996954\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.016030991771259093\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.015872625226728578\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.015714403624707985\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.015552186535792649\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.015395844182409085\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.015276312743967315\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.01509025543413937\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.014950730279301886\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.014811816222213134\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.014674654092689983\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.014518389661521301\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.014379929348532033\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.014264695554542229\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.014121904079239649\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.013992199938380813\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.013876810953989443\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.013752316393136338\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.01361337228619609\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.013497600698501274\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.013381492201550618\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.013257213149367291\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.013139360277602803\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.01303371104838729\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.012919255899269058\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.012810905284353183\n",
      "Accuracy on evaluation data: 8376 / 10000\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.01270614132450864\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.012605286690854992\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.012516620599337223\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.012406209115064543\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.012294027165993164\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.012199542259782408\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.012097803446887902\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.01201056351127706\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.011908667846224002\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.011818037387011465\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.011724366268177721\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.01164810153703184\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.011544061670299099\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.011455777026413459\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.011372523753172409\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.011284121606334407\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.011210571338945187\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.011123138443888549\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.011034016715397688\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.010958358780158573\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.010891052402509092\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.010800708450573373\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.010729830347140177\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.010649375369897997\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.010580074318783208\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.010501212219455427\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.010430858965364621\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.010352724414182777\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.010284994307816668\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.010212246927470423\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.010143601377853556\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.010074400095666201\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.010008945601147642\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.009942237058595753\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.009876771781419246\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.009813137960920358\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.009746187985853826\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.00968794009664353\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.009624216886496319\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.00956097748322995\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.009501839442031635\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.009441419373872354\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.009379733722470713\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.009326223818737135\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.009263771043487717\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.00920468064103416\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.009148814327722113\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.00909299829949708\n",
      "Accuracy on evaluation data: 8389 / 10000\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.009035886496872748\n",
      "Accuracy on evaluation data: 8379 / 10000\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.008981031978552944\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.008929798356904092\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.008874506787066265\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.008818587293846282\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.008765935760337568\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.008714484049987392\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.008661764529492029\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.008612279972288925\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.008559996653040573\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.008509548707517862\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.008457724455268115\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.008408383155317148\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.008359146320980636\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.008311432329093269\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.008262062719235816\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.00821557719853568\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.00816754706336732\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.008124403766404487\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 0.008074223201189026\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.008025686904544126\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.007977847258982013\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.007931537314393685\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.007886508867284318\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.007840912930278037\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.007796483740228169\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.0077531299170985005\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.007707004697260368\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.00766455095838906\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.007621836299061537\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.007578644459776225\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.007536907516740929\n",
      "Accuracy on evaluation data: 8389 / 10000\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.007493339169658076\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.007451995141764757\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.0074103415773679755\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.007371248678295229\n",
      "Accuracy on evaluation data: 8396 / 10000\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.007332288583171439\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.007290726844133697\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.007250521360611109\n",
      "Accuracy on evaluation data: 8389 / 10000\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.0072120843456475415\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.007173856869472073\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.007136759495123127\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.007098383666663315\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.0070617856288683195\n",
      "Accuracy on evaluation data: 8394 / 10000\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.007025082020101826\n",
      "Accuracy on evaluation data: 8389 / 10000\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.006988698070711685\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.006953740748865147\n",
      "Accuracy on evaluation data: 8394 / 10000\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.0069181204087134535\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.006883048624627905\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.006848234544335815\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.006813749573347648\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.006779780139971048\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.006745909329019013\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.006712883349545401\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.0066805759062725425\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.006648163392842197\n",
      "Accuracy on evaluation data: 8389 / 10000\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.006615835503631748\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.00658358844710576\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.006552386651592167\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.006520642200573985\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.006490570363348016\n",
      "Accuracy on evaluation data: 8396 / 10000\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.0064592069901946315\n",
      "Accuracy on evaluation data: 8394 / 10000\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.006428770197402066\n",
      "Accuracy on evaluation data: 8394 / 10000\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.006398350804612856\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.0063688552658475815\n",
      "Accuracy on evaluation data: 8394 / 10000\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.006339177671937065\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.006309444904616804\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.006281068656950578\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.006251754259820191\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.006223391726579233\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.006194847202658026\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.006167245156803358\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.006139258444502969\n",
      "Accuracy on evaluation data: 8395 / 10000\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.006111258249988232\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.006084593759301584\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.006057140473502457\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.006029933350617977\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.006004103777281124\n",
      "Accuracy on evaluation data: 8394 / 10000\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.005977010371649703\n",
      "Accuracy on evaluation data: 8396 / 10000\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.005950901519786237\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.00592520241660792\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.005899142852490016\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.0058734655653120355\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.005848403978148454\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.005822954834216809\n",
      "Accuracy on evaluation data: 8396 / 10000\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.005798133691762196\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.005773528107365361\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.005749603446751295\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.0057244097014196626\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.005700365672874179\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.005676767491464474\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.005652390439994531\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.005628996767328683\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.005605402308096846\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.005582723455869592\n",
      "Accuracy on evaluation data: 8404 / 10000\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.00555905370957401\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.005536060964986924\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.005513645783146041\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.005490874372554091\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.005468858892871599\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.005446442781013654\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.0054245281563554066\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.005403069979746911\n",
      "Accuracy on evaluation data: 8396 / 10000\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.005381565238806412\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.005359232427435095\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.005337791302339071\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.00531682267058297\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.005295483236208793\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.0052745498943629135\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.005254012009913148\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.005233240489787689\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.005212800068629191\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.005192522751655138\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.005172536533711694\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.005153160604672999\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.005132779575151626\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.0051130348686050705\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.0050933608739880155\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.005073921448238875\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.005054528432786781\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.005035562661348525\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.00501630410366528\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.004997779728053082\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.00497874236511947\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.0049603598220137055\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.004942104091600276\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.004923426790000908\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.004905082777044869\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.004887106564309667\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.004869082214212583\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.004851224039574166\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.004834016092614999\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.004816088833088533\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.004798838965386462\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.004781387383464164\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.004763960997131588\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.004746872844593466\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.004730015909842807\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.004713181105539012\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.004696247524480082\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.00467960245418833\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.004663328127660776\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.0046466309000341354\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.004630248972316842\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.004614126709005111\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.004598084255620302\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.004581898623740174\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.004566125219047899\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.004550723122634254\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.004534614178152825\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.004519160544231616\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.004503887451583722\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.0044882556643849126\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.004473121782745828\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.004457842404551492\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.004442970870009696\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.004427705926307787\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.004412898757743211\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.004398055029333814\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.004383384143691648\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.004368774986231326\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.004354133390455412\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.004339778237203225\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.004325447857179933\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.004311106924467555\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.00429724256581218\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 0.004283176956383983\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.004268884503150946\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.004254961378528302\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.0042411498468596185\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.004227294879946682\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.004213543778635469\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.0041998983410052515\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.004186305705663537\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.004173033741520392\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.0041594910817641705\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.004146174004974547\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.004132993870014564\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.004119780733057586\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.004106614513583993\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.004093598910416429\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.0040806348867008894\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.004067664996373759\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.004054907744697784\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.00404188948570174\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.004029260680961778\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.004016601361575042\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.00400385924369813\n",
      "Accuracy on evaluation data: 8399 / 10000\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.003991252220129889\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.00397870238808433\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.00396617100830067\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.003953781861792673\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.003941432322749571\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.003929071257445193\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.003916561473911873\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.003904252115591465\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.0038919011980252053\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.003879760665746009\n",
      "Accuracy on evaluation data: 8404 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [5837,\n",
       "  6819,\n",
       "  7128,\n",
       "  7468,\n",
       "  7808,\n",
       "  7846,\n",
       "  7932,\n",
       "  8017,\n",
       "  8122,\n",
       "  8029,\n",
       "  8138,\n",
       "  8176,\n",
       "  8218,\n",
       "  8214,\n",
       "  8234,\n",
       "  8281,\n",
       "  8222,\n",
       "  8248,\n",
       "  8289,\n",
       "  8291,\n",
       "  8292,\n",
       "  8258,\n",
       "  8310,\n",
       "  8306,\n",
       "  8322,\n",
       "  8322,\n",
       "  8321,\n",
       "  8330,\n",
       "  8323,\n",
       "  8334,\n",
       "  8342,\n",
       "  8338,\n",
       "  8331,\n",
       "  8334,\n",
       "  8342,\n",
       "  8348,\n",
       "  8334,\n",
       "  8351,\n",
       "  8342,\n",
       "  8348,\n",
       "  8333,\n",
       "  8338,\n",
       "  8350,\n",
       "  8344,\n",
       "  8340,\n",
       "  8349,\n",
       "  8341,\n",
       "  8351,\n",
       "  8345,\n",
       "  8357,\n",
       "  8350,\n",
       "  8356,\n",
       "  8354,\n",
       "  8351,\n",
       "  8362,\n",
       "  8359,\n",
       "  8369,\n",
       "  8362,\n",
       "  8359,\n",
       "  8362,\n",
       "  8357,\n",
       "  8363,\n",
       "  8377,\n",
       "  8370,\n",
       "  8367,\n",
       "  8376,\n",
       "  8377,\n",
       "  8375,\n",
       "  8373,\n",
       "  8366,\n",
       "  8373,\n",
       "  8372,\n",
       "  8365,\n",
       "  8363,\n",
       "  8374,\n",
       "  8370,\n",
       "  8361,\n",
       "  8366,\n",
       "  8369,\n",
       "  8369,\n",
       "  8375,\n",
       "  8367,\n",
       "  8372,\n",
       "  8374,\n",
       "  8385,\n",
       "  8377,\n",
       "  8371,\n",
       "  8378,\n",
       "  8373,\n",
       "  8374,\n",
       "  8375,\n",
       "  8384,\n",
       "  8380,\n",
       "  8372,\n",
       "  8384,\n",
       "  8377,\n",
       "  8371,\n",
       "  8379,\n",
       "  8379,\n",
       "  8382,\n",
       "  8374,\n",
       "  8370,\n",
       "  8370,\n",
       "  8370,\n",
       "  8374,\n",
       "  8375,\n",
       "  8371,\n",
       "  8364,\n",
       "  8369,\n",
       "  8375,\n",
       "  8375,\n",
       "  8375,\n",
       "  8373,\n",
       "  8380,\n",
       "  8374,\n",
       "  8377,\n",
       "  8381,\n",
       "  8375,\n",
       "  8373,\n",
       "  8376,\n",
       "  8377,\n",
       "  8375,\n",
       "  8378,\n",
       "  8382,\n",
       "  8381,\n",
       "  8382,\n",
       "  8390,\n",
       "  8378,\n",
       "  8385,\n",
       "  8382,\n",
       "  8385,\n",
       "  8384,\n",
       "  8379,\n",
       "  8384,\n",
       "  8383,\n",
       "  8385,\n",
       "  8378,\n",
       "  8382,\n",
       "  8385,\n",
       "  8380,\n",
       "  8380,\n",
       "  8381,\n",
       "  8380,\n",
       "  8377,\n",
       "  8377,\n",
       "  8382,\n",
       "  8379,\n",
       "  8377,\n",
       "  8380,\n",
       "  8380,\n",
       "  8374,\n",
       "  8375,\n",
       "  8375,\n",
       "  8376,\n",
       "  8378,\n",
       "  8380,\n",
       "  8384,\n",
       "  8377,\n",
       "  8377,\n",
       "  8377,\n",
       "  8378,\n",
       "  8380,\n",
       "  8382,\n",
       "  8380,\n",
       "  8382,\n",
       "  8380,\n",
       "  8383,\n",
       "  8382,\n",
       "  8385,\n",
       "  8380,\n",
       "  8383,\n",
       "  8381,\n",
       "  8380,\n",
       "  8381,\n",
       "  8384,\n",
       "  8383,\n",
       "  8379,\n",
       "  8381,\n",
       "  8384,\n",
       "  8385,\n",
       "  8381,\n",
       "  8384,\n",
       "  8384,\n",
       "  8381,\n",
       "  8386,\n",
       "  8379,\n",
       "  8383,\n",
       "  8385,\n",
       "  8384,\n",
       "  8384,\n",
       "  8384,\n",
       "  8383,\n",
       "  8386,\n",
       "  8379,\n",
       "  8385,\n",
       "  8382,\n",
       "  8381,\n",
       "  8388,\n",
       "  8387,\n",
       "  8381,\n",
       "  8382,\n",
       "  8389,\n",
       "  8379,\n",
       "  8384,\n",
       "  8384,\n",
       "  8385,\n",
       "  8384,\n",
       "  8384,\n",
       "  8390,\n",
       "  8388,\n",
       "  8387,\n",
       "  8388,\n",
       "  8385,\n",
       "  8384,\n",
       "  8388,\n",
       "  8385,\n",
       "  8385,\n",
       "  8385,\n",
       "  8382,\n",
       "  8386,\n",
       "  8384,\n",
       "  8391,\n",
       "  8386,\n",
       "  8388,\n",
       "  8386,\n",
       "  8385,\n",
       "  8385,\n",
       "  8387,\n",
       "  8391,\n",
       "  8388,\n",
       "  8386,\n",
       "  8391,\n",
       "  8390,\n",
       "  8389,\n",
       "  8391,\n",
       "  8392,\n",
       "  8392,\n",
       "  8396,\n",
       "  8392,\n",
       "  8392,\n",
       "  8389,\n",
       "  8387,\n",
       "  8390,\n",
       "  8392,\n",
       "  8391,\n",
       "  8394,\n",
       "  8389,\n",
       "  8388,\n",
       "  8394,\n",
       "  8387,\n",
       "  8390,\n",
       "  8393,\n",
       "  8392,\n",
       "  8392,\n",
       "  8390,\n",
       "  8388,\n",
       "  8391,\n",
       "  8389,\n",
       "  8395,\n",
       "  8395,\n",
       "  8392,\n",
       "  8391,\n",
       "  8396,\n",
       "  8394,\n",
       "  8394,\n",
       "  8393,\n",
       "  8394,\n",
       "  8393,\n",
       "  8395,\n",
       "  8395,\n",
       "  8397,\n",
       "  8395,\n",
       "  8393,\n",
       "  8398,\n",
       "  8395,\n",
       "  8398,\n",
       "  8397,\n",
       "  8397,\n",
       "  8397,\n",
       "  8394,\n",
       "  8396,\n",
       "  8398,\n",
       "  8400,\n",
       "  8397,\n",
       "  8398,\n",
       "  8398,\n",
       "  8396,\n",
       "  8397,\n",
       "  8397,\n",
       "  8399,\n",
       "  8400,\n",
       "  8397,\n",
       "  8401,\n",
       "  8398,\n",
       "  8400,\n",
       "  8401,\n",
       "  8404,\n",
       "  8399,\n",
       "  8401,\n",
       "  8400,\n",
       "  8399,\n",
       "  8397,\n",
       "  8402,\n",
       "  8398,\n",
       "  8396,\n",
       "  8397,\n",
       "  8399,\n",
       "  8397,\n",
       "  8399,\n",
       "  8398,\n",
       "  8401,\n",
       "  8399,\n",
       "  8398,\n",
       "  8399,\n",
       "  8400,\n",
       "  8403,\n",
       "  8403,\n",
       "  8403,\n",
       "  8403,\n",
       "  8400,\n",
       "  8400,\n",
       "  8403,\n",
       "  8400,\n",
       "  8400,\n",
       "  8399,\n",
       "  8400,\n",
       "  8400,\n",
       "  8399,\n",
       "  8400,\n",
       "  8397,\n",
       "  8397,\n",
       "  8400,\n",
       "  8400,\n",
       "  8400,\n",
       "  8401,\n",
       "  8401,\n",
       "  8401,\n",
       "  8400,\n",
       "  8400,\n",
       "  8402,\n",
       "  8402,\n",
       "  8400,\n",
       "  8400,\n",
       "  8399,\n",
       "  8401,\n",
       "  8402,\n",
       "  8398,\n",
       "  8400,\n",
       "  8398,\n",
       "  8399,\n",
       "  8399,\n",
       "  8398,\n",
       "  8398,\n",
       "  8399,\n",
       "  8399,\n",
       "  8398,\n",
       "  8397,\n",
       "  8400,\n",
       "  8399,\n",
       "  8398,\n",
       "  8399,\n",
       "  8398,\n",
       "  8398,\n",
       "  8399,\n",
       "  8400,\n",
       "  8400,\n",
       "  8402,\n",
       "  8398,\n",
       "  8399,\n",
       "  8402,\n",
       "  8403,\n",
       "  8402,\n",
       "  8402,\n",
       "  8402,\n",
       "  8401,\n",
       "  8400,\n",
       "  8403,\n",
       "  8402,\n",
       "  8402,\n",
       "  8403,\n",
       "  8401,\n",
       "  8401,\n",
       "  8400,\n",
       "  8399,\n",
       "  8401,\n",
       "  8399,\n",
       "  8398,\n",
       "  8398,\n",
       "  8399,\n",
       "  8399,\n",
       "  8400,\n",
       "  8400,\n",
       "  8401,\n",
       "  8402,\n",
       "  8402,\n",
       "  8402,\n",
       "  8401,\n",
       "  8401,\n",
       "  8403,\n",
       "  8404],\n",
       " [1.8945396041045133,\n",
       "  1.3953496565015362,\n",
       "  1.1991417703682612,\n",
       "  1.0126872783903407,\n",
       "  0.8419328491914003,\n",
       "  0.775431969469158,\n",
       "  0.6838249069147905,\n",
       "  0.6160513111299628,\n",
       "  0.5444903990460784,\n",
       "  0.5323828173446684,\n",
       "  0.4648742818701191,\n",
       "  0.4141119158372673,\n",
       "  0.3718897159739673,\n",
       "  0.34695173949443264,\n",
       "  0.32335882523801235,\n",
       "  0.30492712483643863,\n",
       "  0.2867598325713026,\n",
       "  0.2692215844397362,\n",
       "  0.24142734479076114,\n",
       "  0.22421913765983556,\n",
       "  0.21894352172935855,\n",
       "  0.20855393429758307,\n",
       "  0.18692501290336042,\n",
       "  0.1782425942950656,\n",
       "  0.1677525617504816,\n",
       "  0.16030858788839275,\n",
       "  0.15406312057681468,\n",
       "  0.1442880006843516,\n",
       "  0.1372163976653251,\n",
       "  0.13162827830505927,\n",
       "  0.12735270544714947,\n",
       "  0.11947483393396191,\n",
       "  0.11477642525570472,\n",
       "  0.11250635610587101,\n",
       "  0.10825633683310873,\n",
       "  0.10215740471515891,\n",
       "  0.09846656851438527,\n",
       "  0.09649603827789266,\n",
       "  0.0912575354567861,\n",
       "  0.08970993846601422,\n",
       "  0.08538456385624618,\n",
       "  0.08121847831481757,\n",
       "  0.07984729518961645,\n",
       "  0.07592777990665636,\n",
       "  0.0735881751747304,\n",
       "  0.07104001715966438,\n",
       "  0.06829898457384864,\n",
       "  0.06610886085224928,\n",
       "  0.06423825224270642,\n",
       "  0.06283370027875229,\n",
       "  0.06060944982781599,\n",
       "  0.05915456132334416,\n",
       "  0.05787387072242858,\n",
       "  0.056380773991726606,\n",
       "  0.05464500578511769,\n",
       "  0.05338360987972593,\n",
       "  0.052689702217370044,\n",
       "  0.05087934430038287,\n",
       "  0.04948735871520646,\n",
       "  0.04830192042471011,\n",
       "  0.04798296543281493,\n",
       "  0.046328549951913794,\n",
       "  0.04539127456704237,\n",
       "  0.04433174255264326,\n",
       "  0.04371379734385616,\n",
       "  0.042525379460731254,\n",
       "  0.04161170296127121,\n",
       "  0.04069434049957627,\n",
       "  0.03970637881424219,\n",
       "  0.03898764333702563,\n",
       "  0.03818297065432241,\n",
       "  0.037455404750470066,\n",
       "  0.03664210200177689,\n",
       "  0.03602104221023731,\n",
       "  0.03533446794152662,\n",
       "  0.03466184270029866,\n",
       "  0.034311124743951496,\n",
       "  0.033433035982893024,\n",
       "  0.03278921041374515,\n",
       "  0.03228441053463231,\n",
       "  0.031639095351728125,\n",
       "  0.03113491388586037,\n",
       "  0.030583750262745305,\n",
       "  0.02999356984401455,\n",
       "  0.029501432791083054,\n",
       "  0.028998006387209806,\n",
       "  0.028507100111852292,\n",
       "  0.02802727496448332,\n",
       "  0.027535194386568297,\n",
       "  0.027045888274652218,\n",
       "  0.026715327922240992,\n",
       "  0.02619613905385415,\n",
       "  0.025796150587489113,\n",
       "  0.02534751213319773,\n",
       "  0.02494083488674298,\n",
       "  0.02461471279594119,\n",
       "  0.024153438756269355,\n",
       "  0.02384150585793765,\n",
       "  0.02348425697619531,\n",
       "  0.023171397130190643,\n",
       "  0.022805204320697475,\n",
       "  0.022468803769767082,\n",
       "  0.02216801632552247,\n",
       "  0.021961938017286183,\n",
       "  0.021557009158748298,\n",
       "  0.021262581934206794,\n",
       "  0.020996247079719723,\n",
       "  0.020737781693415735,\n",
       "  0.020531688089895483,\n",
       "  0.020302760076465334,\n",
       "  0.019980651504799243,\n",
       "  0.019682623865699876,\n",
       "  0.019449402088868625,\n",
       "  0.019235425505213632,\n",
       "  0.019002439448171735,\n",
       "  0.01875273510571845,\n",
       "  0.01852292033719572,\n",
       "  0.018330783759363537,\n",
       "  0.01811080517027363,\n",
       "  0.01792685851519484,\n",
       "  0.017710344396465037,\n",
       "  0.017490910924275432,\n",
       "  0.017304514733114514,\n",
       "  0.01710475519489045,\n",
       "  0.016918810369400822,\n",
       "  0.0167308887409212,\n",
       "  0.0165635764686611,\n",
       "  0.016389108971073926,\n",
       "  0.016209723713996954,\n",
       "  0.016030991771259093,\n",
       "  0.015872625226728578,\n",
       "  0.015714403624707985,\n",
       "  0.015552186535792649,\n",
       "  0.015395844182409085,\n",
       "  0.015276312743967315,\n",
       "  0.01509025543413937,\n",
       "  0.014950730279301886,\n",
       "  0.014811816222213134,\n",
       "  0.014674654092689983,\n",
       "  0.014518389661521301,\n",
       "  0.014379929348532033,\n",
       "  0.014264695554542229,\n",
       "  0.014121904079239649,\n",
       "  0.013992199938380813,\n",
       "  0.013876810953989443,\n",
       "  0.013752316393136338,\n",
       "  0.01361337228619609,\n",
       "  0.013497600698501274,\n",
       "  0.013381492201550618,\n",
       "  0.013257213149367291,\n",
       "  0.013139360277602803,\n",
       "  0.01303371104838729,\n",
       "  0.012919255899269058,\n",
       "  0.012810905284353183,\n",
       "  0.01270614132450864,\n",
       "  0.012605286690854992,\n",
       "  0.012516620599337223,\n",
       "  0.012406209115064543,\n",
       "  0.012294027165993164,\n",
       "  0.012199542259782408,\n",
       "  0.012097803446887902,\n",
       "  0.01201056351127706,\n",
       "  0.011908667846224002,\n",
       "  0.011818037387011465,\n",
       "  0.011724366268177721,\n",
       "  0.01164810153703184,\n",
       "  0.011544061670299099,\n",
       "  0.011455777026413459,\n",
       "  0.011372523753172409,\n",
       "  0.011284121606334407,\n",
       "  0.011210571338945187,\n",
       "  0.011123138443888549,\n",
       "  0.011034016715397688,\n",
       "  0.010958358780158573,\n",
       "  0.010891052402509092,\n",
       "  0.010800708450573373,\n",
       "  0.010729830347140177,\n",
       "  0.010649375369897997,\n",
       "  0.010580074318783208,\n",
       "  0.010501212219455427,\n",
       "  0.010430858965364621,\n",
       "  0.010352724414182777,\n",
       "  0.010284994307816668,\n",
       "  0.010212246927470423,\n",
       "  0.010143601377853556,\n",
       "  0.010074400095666201,\n",
       "  0.010008945601147642,\n",
       "  0.009942237058595753,\n",
       "  0.009876771781419246,\n",
       "  0.009813137960920358,\n",
       "  0.009746187985853826,\n",
       "  0.00968794009664353,\n",
       "  0.009624216886496319,\n",
       "  0.00956097748322995,\n",
       "  0.009501839442031635,\n",
       "  0.009441419373872354,\n",
       "  0.009379733722470713,\n",
       "  0.009326223818737135,\n",
       "  0.009263771043487717,\n",
       "  0.00920468064103416,\n",
       "  0.009148814327722113,\n",
       "  0.00909299829949708,\n",
       "  0.009035886496872748,\n",
       "  0.008981031978552944,\n",
       "  0.008929798356904092,\n",
       "  0.008874506787066265,\n",
       "  0.008818587293846282,\n",
       "  0.008765935760337568,\n",
       "  0.008714484049987392,\n",
       "  0.008661764529492029,\n",
       "  0.008612279972288925,\n",
       "  0.008559996653040573,\n",
       "  0.008509548707517862,\n",
       "  0.008457724455268115,\n",
       "  0.008408383155317148,\n",
       "  0.008359146320980636,\n",
       "  0.008311432329093269,\n",
       "  0.008262062719235816,\n",
       "  0.00821557719853568,\n",
       "  0.00816754706336732,\n",
       "  0.008124403766404487,\n",
       "  0.008074223201189026,\n",
       "  0.008025686904544126,\n",
       "  0.007977847258982013,\n",
       "  0.007931537314393685,\n",
       "  0.007886508867284318,\n",
       "  0.007840912930278037,\n",
       "  0.007796483740228169,\n",
       "  0.0077531299170985005,\n",
       "  0.007707004697260368,\n",
       "  0.00766455095838906,\n",
       "  0.007621836299061537,\n",
       "  0.007578644459776225,\n",
       "  0.007536907516740929,\n",
       "  0.007493339169658076,\n",
       "  0.007451995141764757,\n",
       "  0.0074103415773679755,\n",
       "  0.007371248678295229,\n",
       "  0.007332288583171439,\n",
       "  0.007290726844133697,\n",
       "  0.007250521360611109,\n",
       "  0.0072120843456475415,\n",
       "  0.007173856869472073,\n",
       "  0.007136759495123127,\n",
       "  0.007098383666663315,\n",
       "  0.0070617856288683195,\n",
       "  0.007025082020101826,\n",
       "  0.006988698070711685,\n",
       "  0.006953740748865147,\n",
       "  0.0069181204087134535,\n",
       "  0.006883048624627905,\n",
       "  0.006848234544335815,\n",
       "  0.006813749573347648,\n",
       "  0.006779780139971048,\n",
       "  0.006745909329019013,\n",
       "  0.006712883349545401,\n",
       "  0.0066805759062725425,\n",
       "  0.006648163392842197,\n",
       "  0.006615835503631748,\n",
       "  0.00658358844710576,\n",
       "  0.006552386651592167,\n",
       "  0.006520642200573985,\n",
       "  0.006490570363348016,\n",
       "  0.0064592069901946315,\n",
       "  0.006428770197402066,\n",
       "  0.006398350804612856,\n",
       "  0.0063688552658475815,\n",
       "  0.006339177671937065,\n",
       "  0.006309444904616804,\n",
       "  0.006281068656950578,\n",
       "  0.006251754259820191,\n",
       "  0.006223391726579233,\n",
       "  0.006194847202658026,\n",
       "  0.006167245156803358,\n",
       "  0.006139258444502969,\n",
       "  0.006111258249988232,\n",
       "  0.006084593759301584,\n",
       "  0.006057140473502457,\n",
       "  0.006029933350617977,\n",
       "  0.006004103777281124,\n",
       "  0.005977010371649703,\n",
       "  0.005950901519786237,\n",
       "  0.00592520241660792,\n",
       "  0.005899142852490016,\n",
       "  0.0058734655653120355,\n",
       "  0.005848403978148454,\n",
       "  0.005822954834216809,\n",
       "  0.005798133691762196,\n",
       "  0.005773528107365361,\n",
       "  0.005749603446751295,\n",
       "  0.0057244097014196626,\n",
       "  0.005700365672874179,\n",
       "  0.005676767491464474,\n",
       "  0.005652390439994531,\n",
       "  0.005628996767328683,\n",
       "  0.005605402308096846,\n",
       "  0.005582723455869592,\n",
       "  0.00555905370957401,\n",
       "  0.005536060964986924,\n",
       "  0.005513645783146041,\n",
       "  0.005490874372554091,\n",
       "  0.005468858892871599,\n",
       "  0.005446442781013654,\n",
       "  0.0054245281563554066,\n",
       "  0.005403069979746911,\n",
       "  0.005381565238806412,\n",
       "  0.005359232427435095,\n",
       "  0.005337791302339071,\n",
       "  0.00531682267058297,\n",
       "  0.005295483236208793,\n",
       "  0.0052745498943629135,\n",
       "  0.005254012009913148,\n",
       "  0.005233240489787689,\n",
       "  0.005212800068629191,\n",
       "  0.005192522751655138,\n",
       "  0.005172536533711694,\n",
       "  0.005153160604672999,\n",
       "  0.005132779575151626,\n",
       "  0.0051130348686050705,\n",
       "  0.0050933608739880155,\n",
       "  0.005073921448238875,\n",
       "  0.005054528432786781,\n",
       "  0.005035562661348525,\n",
       "  0.00501630410366528,\n",
       "  0.004997779728053082,\n",
       "  0.00497874236511947,\n",
       "  0.0049603598220137055,\n",
       "  0.004942104091600276,\n",
       "  0.004923426790000908,\n",
       "  0.004905082777044869,\n",
       "  0.004887106564309667,\n",
       "  0.004869082214212583,\n",
       "  0.004851224039574166,\n",
       "  0.004834016092614999,\n",
       "  0.004816088833088533,\n",
       "  0.004798838965386462,\n",
       "  0.004781387383464164,\n",
       "  0.004763960997131588,\n",
       "  0.004746872844593466,\n",
       "  0.004730015909842807,\n",
       "  0.004713181105539012,\n",
       "  0.004696247524480082,\n",
       "  0.00467960245418833,\n",
       "  0.004663328127660776,\n",
       "  0.0046466309000341354,\n",
       "  0.004630248972316842,\n",
       "  0.004614126709005111,\n",
       "  0.004598084255620302,\n",
       "  0.004581898623740174,\n",
       "  0.004566125219047899,\n",
       "  0.004550723122634254,\n",
       "  0.004534614178152825,\n",
       "  0.004519160544231616,\n",
       "  0.004503887451583722,\n",
       "  0.0044882556643849126,\n",
       "  0.004473121782745828,\n",
       "  0.004457842404551492,\n",
       "  0.004442970870009696,\n",
       "  0.004427705926307787,\n",
       "  0.004412898757743211,\n",
       "  0.004398055029333814,\n",
       "  0.004383384143691648,\n",
       "  0.004368774986231326,\n",
       "  0.004354133390455412,\n",
       "  0.004339778237203225,\n",
       "  0.004325447857179933,\n",
       "  0.004311106924467555,\n",
       "  0.00429724256581218,\n",
       "  0.004283176956383983,\n",
       "  0.004268884503150946,\n",
       "  0.004254961378528302,\n",
       "  0.0042411498468596185,\n",
       "  0.004227294879946682,\n",
       "  0.004213543778635469,\n",
       "  0.0041998983410052515,\n",
       "  0.004186305705663537,\n",
       "  0.004173033741520392,\n",
       "  0.0041594910817641705,\n",
       "  0.004146174004974547,\n",
       "  0.004132993870014564,\n",
       "  0.004119780733057586,\n",
       "  0.004106614513583993,\n",
       "  0.004093598910416429,\n",
       "  0.0040806348867008894,\n",
       "  0.004067664996373759,\n",
       "  0.004054907744697784,\n",
       "  0.00404188948570174,\n",
       "  0.004029260680961778,\n",
       "  0.004016601361575042,\n",
       "  0.00400385924369813,\n",
       "  0.003991252220129889,\n",
       "  0.00397870238808433,\n",
       "  0.00396617100830067,\n",
       "  0.003953781861792673,\n",
       "  0.003941432322749571,\n",
       "  0.003929071257445193,\n",
       "  0.003916561473911873,\n",
       "  0.003904252115591465,\n",
       "  0.0038919011980252053,\n",
       "  0.003879760665746009],\n",
       " [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = network2.Network(([784, 30, 10]), cost=network2.CrossEntropyCost) #30 hidden layers for Cross Entropy Cost function\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True, monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with more than one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9219 / 10000\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9460 / 10000\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9562 / 10000\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9601 / 10000\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9647 / 10000\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9638 / 10000\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9644 / 10000\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9661 / 10000\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9667 / 10000\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9660 / 10000\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9680 / 10000\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9679 / 10000\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9644 / 10000\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9682 / 10000\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9682 / 10000\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9699 / 10000\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9668 / 10000\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9678 / 10000\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9688 / 10000\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9667 / 10000\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9683 / 10000\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9696 / 10000\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9710 / 10000\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9687 / 10000\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9688 / 10000\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9717 / 10000\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9716 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9219,\n",
       "  9460,\n",
       "  9562,\n",
       "  9571,\n",
       "  9601,\n",
       "  9610,\n",
       "  9647,\n",
       "  9638,\n",
       "  9644,\n",
       "  9661,\n",
       "  9667,\n",
       "  9610,\n",
       "  9660,\n",
       "  9680,\n",
       "  9679,\n",
       "  9644,\n",
       "  9682,\n",
       "  9682,\n",
       "  9699,\n",
       "  9668,\n",
       "  9678,\n",
       "  9688,\n",
       "  9667,\n",
       "  9683,\n",
       "  9696,\n",
       "  9710,\n",
       "  9687,\n",
       "  9688,\n",
       "  9717,\n",
       "  9716],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import network2\n",
    "\n",
    "\n",
    "net = network2.Network([784, 30, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks using Theano library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "c:\\Users\\David\\Desktop\\programming_projects\\mnist_NN\\.venv\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Failed to import scipy.linalg.blas, and Theano flag blas.ldflags is empty. Falling back on slower implementations for dot(matrix, vector), dot(vector, matrix) and dot(vector, vector) (numpy.core.multiarray failed to import)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
      "to set the GPU flag to False.\n",
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Epoch 0: validation accuracy 92.73%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 92.04%\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Epoch 1: validation accuracy 94.71%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 94.26%\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Epoch 2: validation accuracy 95.72%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.39%\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n"
     ]
    }
   ],
   "source": [
    "from src import network3\n",
    "from src.network3 import Network\n",
    "from src.network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "training_data, validation_data, test_data = network3.load_data_shared()\n",
    "mini_batch_size = 10\n",
    "net = Network([FullyConnectedLayer(n_in=784, n_out=100),SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                      filter_shape=(20, 1, 5, 5), \n",
    "                      poolsize=(2, 2)),\n",
    "        FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
    "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
    "            validation_data, test_data)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
